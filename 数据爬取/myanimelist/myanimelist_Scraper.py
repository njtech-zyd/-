"""
MyAnimeList å¤šå­£åº¦çˆ¬è™« - å¢å¼ºç‰ˆ
æ”¯æŒæ–­ç‚¹ç»­ä¼ ã€è‡ªåŠ¨é‡è¯•ã€å®æ—¶ä¿å­˜
"""
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from datetime import datetime
import re
import os
import json
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry


class RobustMALScraper:
    AOT_SEASONS = {
        'Season_3_Part_1': 'https://myanimelist.net/anime/35760/Shingeki_no_Kyojin_Season_3/reviews',
        'Season_3_Part_2': 'https://myanimelist.net/anime/38524/Shingeki_no_Kyojin_Season_3_Part_2/reviews',
    }
    
    def __init__(self, output_dir='mal_data'):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
        }
        
        self.all_reviews = []
        self.progress_file = os.path.join(output_dir, 'progress.json')
        self.progress = self.load_progress()
    
    def create_session(self):
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„session"""
        session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=5,  # æ€»å…±é‡è¯•5æ¬¡
            backoff_factor=2,  # æŒ‡æ•°é€€é¿
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        session.headers.update(self.headers)
        
        return session
    
    def load_progress(self):
        """åŠ è½½è¿›åº¦"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}
    
    def save_progress(self, season, page):
        """ä¿å­˜è¿›åº¦"""
        self.progress[season] = page
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(self.progress, f)
    
    def parse_review(self, review_div, season_name):
        """è§£æè¯„è®º"""
        try:
            data = {}
            
            # ç”¨æˆ·å
            username_elem = review_div.find('div', class_='username')
            if username_elem:
                user_link = username_elem.find('a')
                data['username'] = user_link.text.strip() if user_link else 'Unknown'
            else:
                data['username'] = 'Unknown'
            
            # è¯„è®ºæ–‡æœ¬
            text_elem = review_div.find('div', class_='text')
            if text_elem:
                for unwanted in text_elem.find_all(['a', 'div']):
                    unwanted.decompose()
                data['comment'] = text_elem.get_text(separator=' ', strip=True)
            else:
                data['comment'] = review_div.get_text(separator=' ', strip=True)
            
            if len(data['comment']) < 20:
                return None
            
            # è¯„åˆ†
            all_text = review_div.get_text()
            
            overall_elem = review_div.find('div', class_='rating')
            if overall_elem:
                match = re.search(r'(\d+)', overall_elem.get_text(strip=True))
                data['rating'] = match.group(1) if match else 'N/A'
            else:
                data['rating'] = 'N/A'
            
            # æ—¥æœŸ
            date_elem = review_div.find('div', class_='date')
            if date_elem:
                data['time'] = date_elem.get_text(strip=True)
            else:
                date_match = re.search(r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d+,\s+\d{4}', all_text)
                data['time'] = date_match.group(0) if date_match else 'N/A'
            
            # æœ‰ç”¨åº¦
            helpful_match = re.search(r'(\d+)\s+of\s+(\d+)', all_text)
            if helpful_match:
                data['votes'] = f"{helpful_match.group(1)}/{helpful_match.group(2)}"
            else:
                data['votes'] = '0/0'
            
            # å…ƒæ•°æ®
            data['season'] = season_name
            data['scraped_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            return data
            
        except Exception as e:
            return None
    
    def scrape_page_with_retry(self, url, page_num, season_name, max_retries=3):
        """çˆ¬å–å•é¡µï¼ˆå¸¦é‡è¯•ï¼‰"""
        for attempt in range(max_retries):
            try:
                session = self.create_session()
                response = session.get(url, timeout=30)
                
                if response.status_code != 200:
                    if attempt < max_retries - 1:
                        time.sleep(5 * (attempt + 1))
                        continue
                    return 0
                
                soup = BeautifulSoup(response.text, 'html.parser')
                review_elements = soup.find_all('div', class_='review-element')
                
                if not review_elements:
                    return 0
                
                page_reviews = []
                for elem in review_elements:
                    review_data = self.parse_review(elem, season_name)
                    if review_data:
                        page_reviews.append(review_data)
                
                return page_reviews
                
            except (requests.exceptions.SSLError, 
                    requests.exceptions.ConnectionError,
                    requests.exceptions.Timeout) as e:
                print(f"      âš ï¸ å°è¯• {attempt+1}/{max_retries}: {type(e).__name__}")
                if attempt < max_retries - 1:
                    wait_time = 10 * (attempt + 1)
                    print(f"      â¸ï¸  ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"      âŒ ç¬¬{page_num}é¡µå¤±è´¥ï¼Œè·³è¿‡")
                    return 0
            except KeyboardInterrupt:
                print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å½“å‰æ•°æ®...")
                self.save_current_data()
                raise
            except Exception as e:
                print(f"      âŒ æœªçŸ¥é”™è¯¯: {e}")
                return 0
        
        return 0
    
    def scrape_season(self, season_name, url, max_pages=30):
        """çˆ¬å–å•ä¸ªå­£åº¦"""
        print(f"\n{'='*70}")
        print(f"ğŸ¬ å¼€å§‹çˆ¬å–: {season_name}")
        print(f"{'='*70}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–­ç‚¹
        start_page = self.progress.get(season_name, 1)
        if start_page > 1:
            print(f"ğŸ“ ä»ç¬¬ {start_page} é¡µç»§ç»­...")
        
        season_reviews = []
        consecutive_empty = 0
        
        for page in range(start_page, max_pages + 1):
            page_url = url if page == 1 else f"{url}?p={page}"
            
            print(f"ğŸ“„ ç¬¬{page}é¡µ...", end=' ')
            
            page_reviews = self.scrape_page_with_retry(page_url, page, season_name)
            
            if isinstance(page_reviews, list) and len(page_reviews) > 0:
                season_reviews.extend(page_reviews)
                print(f"âœ… {len(page_reviews)}æ¡ (ç´¯è®¡: {len(season_reviews)})")
                consecutive_empty = 0
                
                # ä¿å­˜è¿›åº¦
                self.save_progress(season_name, page)
                
                # æ¯10é¡µä¿å­˜ä¸€æ¬¡æ•°æ®
                if page % 10 == 0:
                    self.save_season_data(season_name, season_reviews)
            else:
                print(f"âŒ 0æ¡")
                consecutive_empty += 1
                if consecutive_empty >= 3:
                    print(f"      ğŸ›‘ è¿ç»­ {consecutive_empty} é¡µæ— æ•°æ®ï¼Œåœæ­¢æ­¤å­£")
                    break
            
            # å»¶è¿Ÿ
            time.sleep(random.uniform(2, 5))
        
        # ä¿å­˜æœ€ç»ˆæ•°æ®
        self.save_season_data(season_name, season_reviews)
        
        print(f"âœ¨ {season_name} å®Œæˆ: {len(season_reviews)}æ¡è¯„è®º\n")
        return season_reviews
    
    def save_season_data(self, season_name, reviews):
        """ä¿å­˜å•å­£æ•°æ®"""
        if not reviews:
            return
        
        # åªä¿ç•™éœ€è¦çš„å­—æ®µ
        filtered_reviews = []
        for review in reviews:
            filtered_review = {
                'username': review.get('username', 'Unknown'),
                'rating': review.get('rating', 'N/A'),
                'time': review.get('time', 'N/A'),
                'votes': review.get('votes', '0/0'),
                'comment': review.get('comment', ''),
                'season': review.get('season', '')
            }
            filtered_reviews.append(filtered_review)
        
        df = pd.DataFrame(filtered_reviews)
        timestamp = datetime.now().strftime('%Y%m%d')
        filename = os.path.join(self.output_dir, f'{season_name}_{timestamp}.csv')
        
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"      ğŸ’¾ å·²ä¿å­˜: {filename}")
    
    def save_current_data(self):
        """ä¿å­˜å½“å‰æ‰€æœ‰æ•°æ®"""
        if not self.all_reviews:
            return
        
        # åªä¿ç•™éœ€è¦çš„å­—æ®µ
        filtered_reviews = []
        for review in self.all_reviews:
            filtered_review = {
                'username': review.get('username', 'Unknown'),
                'rating': review.get('rating', 'N/A'),
                'time': review.get('time', 'N/A'),
                'votes': review.get('votes', '0/0'),
                'comment': review.get('comment', ''),
                'season': review.get('season', '')
            }
            filtered_reviews.append(filtered_review)
        
        df = pd.DataFrame(filtered_reviews)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = os.path.join(self.output_dir, f'AOT_PARTIAL_{timestamp}.csv')
        
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ å·²ä¿å­˜å½“å‰æ•°æ®: {filename}")
        print(f"ğŸ“Š å…± {len(df)} æ¡è¯„è®º")
    
    def scrape_all_seasons(self):
        """çˆ¬å–æ‰€æœ‰å­£åº¦"""
        print("\n" + "="*70)
        print("ğŸš€ MyAnimeList è¿›å‡»çš„å·¨äººå…¨ç³»åˆ—çˆ¬è™« (å¢å¼ºç‰ˆ)")
        print("="*70)
        print(f"ğŸ“º ç›®æ ‡å­£åº¦: {len(self.AOT_SEASONS)}ä¸ª")
        print(f"ğŸ’¾ æ•°æ®ä¿å­˜ç›®å½•: {self.output_dir}")
        print("ğŸ”„ æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œè‡ªåŠ¨é‡è¯•")
        print("="*70)
        
        start_time = time.time()
        
        try:
            for i, (season_name, url) in enumerate(self.AOT_SEASONS.items(), 1):
                print(f"\n[{i}/{len(self.AOT_SEASONS)}] {season_name}")
                
                season_reviews = self.scrape_season(season_name, url)
                self.all_reviews.extend(season_reviews)
                
                # å­£ä¹‹é—´å»¶è¿Ÿ
                if i < len(self.AOT_SEASONS):
                    wait = random.uniform(10, 20)
                    print(f"â¸ï¸  ç­‰å¾… {wait:.1f} ç§’åç»§ç»­ä¸‹ä¸€å­£...\n")
                    time.sleep(wait)
        
        except KeyboardInterrupt:
            print("\n\nâš ï¸ çˆ¬å–è¢«ä¸­æ–­")
            self.save_current_data()
            return self.all_reviews
        
        elapsed = time.time() - start_time
        
        print("\n" + "="*70)
        print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è¯„è®ºæ•°: {len(self.all_reviews)}")
        print(f"â±ï¸  æ€»è€—æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ")
        print("="*70)
        
        return self.all_reviews
    
    def save_final_data(self):
        """ä¿å­˜æœ€ç»ˆæ•°æ®"""
        if not self.all_reviews:
            print("âš ï¸ æ²¡æœ‰æ•°æ®")
            return None
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # åªä¿ç•™éœ€è¦çš„å­—æ®µ
        filtered_reviews = []
        for review in self.all_reviews:
            filtered_review = {
                'username': review.get('username', 'Unknown'),
                'rating': review.get('rating', 'N/A'),
                'time': review.get('time', 'N/A'),
                'votes': review.get('votes', '0/0'),
                'comment': review.get('comment', ''),
                'season': review.get('season', '')
            }
            filtered_reviews.append(filtered_review)
        
        df = pd.DataFrame(filtered_reviews)
        
        # CSV
        csv_file = os.path.join(self.output_dir, f'AOT_ALL_SEASONS_{timestamp}.csv')
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ æ€»æ•°æ®CSV: {csv_file}")
        
        # Excel
        excel_file = os.path.join(self.output_dir, f'AOT_ALL_SEASONS_{timestamp}.xlsx')
        df.to_excel(excel_file, index=False, engine='openpyxl')
        print(f"ğŸ’¾ æ€»æ•°æ®Excel: {excel_file}")
        
        # ç»Ÿè®¡æŠ¥å‘Š
        self.generate_report(df, timestamp)
        
        # æ¸…é™¤è¿›åº¦æ–‡ä»¶
        if os.path.exists(self.progress_file):
            os.remove(self.progress_file)
            print(f"ğŸ—‘ï¸  å·²æ¸…é™¤è¿›åº¦æ–‡ä»¶")
        
        return df
    
    def generate_report(self, df, timestamp):
        """ç”ŸæˆæŠ¥å‘Š"""
        report_file = os.path.join(self.output_dir, f'REPORT_{timestamp}.txt')
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("="*70 + "\n")
            f.write("è¿›å‡»çš„å·¨äºº MyAnimeList è¯„è®ºæ•°æ®ç»Ÿè®¡æŠ¥å‘Š\n")
            f.write("="*70 + "\n\n")
            
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write(f"ã€æ€»ä½“ç»Ÿè®¡ã€‘\n")
            f.write(f"æ€»è¯„è®ºæ•°: {len(df):,}\n")
            f.write(f"æ¶µç›–å­£åº¦: {df['season'].nunique()}\n")
            f.write(f"ç‹¬ç«‹ç”¨æˆ·: {df['username'].nunique():,}\n")
            f.write(f"å¹³å‡è¯„è®ºé•¿åº¦: {df['comment'].str.len().mean():.0f} å­—ç¬¦\n\n")
            
            f.write(f"ã€å„å­£åº¦è¯„è®ºæ•°ã€‘\n")
            for season, count in df['season'].value_counts().items():
                f.write(f"{season:25s}: {count:4d}\n")
            f.write("\n")
            
            f.write(f"ã€è¯„åˆ†åˆ†å¸ƒã€‘\n")
            ratings = df[df['rating'] != 'N/A']['rating'].value_counts().sort_index()
            for rating, count in ratings.items():
                f.write(f"{rating}åˆ†: {count:4d}\n")
        
        print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š: {report_file}")


def main():
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                              â•‘
    â•‘      MyAnimeList è¿›å‡»çš„å·¨äººå…¨ç³»åˆ—è¯„è®ºçˆ¬è™« - å¢å¼ºç‰ˆ          â•‘
    â•‘      æ”¯æŒæ–­ç‚¹ç»­ä¼  | è‡ªåŠ¨é‡è¯• | å®æ—¶ä¿å­˜                      â•‘
    â•‘                                                              â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    print("\nğŸ’¡ æ–°åŠŸèƒ½:")
    print("  âœ… è‡ªåŠ¨é‡è¯•å¤±è´¥çš„é¡µé¢")
    print("  âœ… é‡åˆ°ä¸­æ–­å¯ä»¥ç»§ç»­ï¼ˆä¿å­˜è¿›åº¦ï¼‰")
    print("  âœ… æ¯10é¡µè‡ªåŠ¨ä¿å­˜æ•°æ®")
    print("  âœ… å¢å¼ºçš„SSLé”™è¯¯å¤„ç†")
    
    choice = input("\né€‰æ‹©æ¨¡å¼:\n1. çˆ¬å–æ‰€æœ‰å­£åº¦\n2. åªçˆ¬å–æŒ‡å®šå­£åº¦\nè¯·è¾“å…¥(1/2): ").strip()
    
    scraper = RobustMALScraper()
    
    if choice == '2':
        print("\nå¯ç”¨å­£åº¦:")
        seasons = list(scraper.AOT_SEASONS.keys())
        for i, s in enumerate(seasons, 1):
            print(f"{i}. {s}")
        
        selected = input("\nè¾“å…¥ç¼–å·(é€—å·åˆ†éš”): ").strip()
        indices = [int(x.strip())-1 for x in selected.split(',') if x.strip().isdigit()]
        
        selected_seasons = {seasons[i]: scraper.AOT_SEASONS[seasons[i]] 
                          for i in indices if 0 <= i < len(seasons)}
        scraper.AOT_SEASONS = selected_seasons
    
    print("\n" + "="*70)
    print(f"å°†çˆ¬å– {len(scraper.AOT_SEASONS)} ä¸ªå­£åº¦")
    input("\næŒ‰ Enter å¼€å§‹...")
    
    try:
        # å¼€å§‹çˆ¬å–
        reviews = scraper.scrape_all_seasons()
        
        # ä¿å­˜æœ€ç»ˆæ•°æ®
        if reviews:
            df = scraper.save_final_data()
            
            print(f"\nâœ… å®Œæˆï¼")
            print(f"ğŸ“ æ‰€æœ‰æ•°æ®ä¿å­˜åœ¨: {scraper.output_dir}")
            print(f"ğŸ“Š æ€»è®¡: {len(df)} æ¡è¯„è®º")
    
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç¨‹åºå·²ç»ˆæ­¢")
        print(f"ğŸ’¾ å·²ä¿å­˜çš„æ•°æ®åœ¨: {scraper.output_dir}")
        print(f"ğŸ’¡ ä¸‹æ¬¡è¿è¡Œä¼šä»ä¸­æ–­å¤„ç»§ç»­")


if __name__ == "__main__":
    main()